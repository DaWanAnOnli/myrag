#!/usr/bin/env python3
"""
Aggregator Decision Analysis Script
Extracts aggregator decisions from multi-agent RAG log files, maps to LLM judge scores,
and generates analysis reports.
"""

import os
import re
import csv
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

# ============ CONFIGURATION ============
# Path to the CSV file with LLM judge results
CSV_FILE = Path("../../../dataset/4_experiment/4c_experiment_results/new/Individual base multi-agent - approach_1_both.csv")

# Layer 1 folder containing the layer 2 experiment folders
LAYER1_FOLDER = Path("../../4b_retrieval/4b_iv_multi_agents/question_terminal_logs_multi_agent/no_11_approach_1_both")


# Mapping of folder name to CSV column name
FOLDER_TO_COLUMN = {
    "no_11_approach_1_both_5_hops_1250": "approach_1_both_answer score",
}

# Output directory for results
OUTPUT_DIR = Path("results_11_both")

# Valid aggregator decisions
VALID_DECISIONS = {"choose_graphrag", "choose_naiverag", "merge"}


# ============ HELPER FUNCTIONS ============
def extract_question_id_from_filename(filename: str) -> int:
    """
    Extract question ID from log filename.
    Example: q0909_wid8_id908_20250927-102245_According...
    Returns: 908
    """
    match = re.search(r'_id(\d+)_', filename)
    if match:
        return int(match.group(1))
    return -1


def extract_aggregator_decision_from_log(log_path: Path) -> str:
    """
    Extract the aggregator decision from a multi-agent RAG log file.
    Uses multiple methods with most reliable patterns preferred.
    
    Returns one of: "choose_graphrag", "choose_naiverag", "merge", or "unknown"
    """
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Method 1: Look for "[Aggregator] Decision=<decision>" (most reliable, from aggregator agent)
        match = re.search(r'\[Aggregator\]\s+Decision\s*=\s*(\w+)', content, re.IGNORECASE)
        if match:
            decision = match.group(1).strip().lower()
            if decision in VALID_DECISIONS:
                return decision
        
        # Method 2: Look for "- Aggregator decision: <decision>" in summary section
        match = re.search(r'-\s+Aggregator\s+decision:\s+(\w+)', content, re.IGNORECASE)
        if match:
            decision = match.group(1).strip().lower()
            if decision in VALID_DECISIONS:
                return decision
        
        # Method 3: Look for '"decision": "<decision>"' in aggregator output (JSON-like)
        match = re.search(r'"decision"\s*:\s*"(choose_graphrag|choose_naiverag|merge)"', content, re.IGNORECASE)
        if match:
            decision = match.group(1).strip().lower()
            if decision in VALID_DECISIONS:
                return decision
        
        # Method 4: Look for fallback messages
        if re.search(r'GraphRAG\s+failed.*Using\s+NaiveRAG', content, re.IGNORECASE):
            return "choose_naiverag"
        if re.search(r'NaiveRAG\s+failed.*Using\s+GraphRAG', content, re.IGNORECASE):
            return "choose_graphrag"
            
    except Exception as e:
        print(f"Error reading {log_path}: {e}")
    
    return "unknown"


def load_scores_from_csv(csv_path: Path) -> Dict[str, Dict[int, int]]:
    """
    Load scores from CSV file.
    Returns: {column_name: {question_id: score}}
    """
    scores = {}
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            # Initialize score dictionaries for each column
            for col_name in FOLDER_TO_COLUMN.values():
                scores[col_name] = {}
            
            row_count = 0
            for row in reader:
                row_count += 1
                try:
                    q_id = int(row['id'])
                    for col_name in FOLDER_TO_COLUMN.values():
                        try:
                            score_str = row[col_name].strip()
                            score = int(score_str)
                            scores[col_name][q_id] = score
                        except (ValueError, KeyError) as e:
                            scores[col_name][q_id] = -1  # Missing or invalid score
                except (ValueError, KeyError) as e:
                    print(f"Warning: Error processing row {row_count}: {e}")
                    continue
            
            print(f"Loaded {row_count} rows from CSV")
    except Exception as e:
        print(f"Error loading CSV file: {e}")
        raise
    
    return scores


def process_experiment_folder(folder_path: Path, column_name: str, 
                              scores: Dict[int, int]) -> List[Tuple[int, str, int]]:
    """
    Process all log files in an experiment folder.
    Returns: List of (question_id, aggregator_decision, score) tuples
    """
    results = []
    failed_extractions = []
    
    # Find all .txt log files (multi-agent logs typically end with .multi-agent.txt)
    log_files = list(folder_path.glob("*.txt"))
    print(f"  Found {len(log_files)} log files in {folder_path.name}")
    
    for log_file in log_files:
        # Extract question ID from filename
        q_id = extract_question_id_from_filename(log_file.name)
        if q_id == -1:
            print(f"  Warning: Could not extract ID from {log_file.name}")
            continue
        
        # Extract aggregator decision from log content
        decision = extract_aggregator_decision_from_log(log_file)
        if decision == "unknown":
            failed_extractions.append((q_id, log_file.name))
            continue
        
        # Get score from CSV
        score = scores.get(q_id, -1)
        if score == -1:
            print(f"  Warning: No score found for question ID {q_id}")
        
        results.append((q_id, decision, score))
    
    if failed_extractions:
        print(f"  WARNING: Failed to extract aggregator decision from {len(failed_extractions)} files:")
        for q_id, fname in failed_extractions[:5]:  # Show first 5
            print(f"    - Question {q_id}: {fname}")
        if len(failed_extractions) > 5:
            print(f"    ... and {len(failed_extractions) - 5} more")
    
    return results


def analyze_results(results: List[Tuple[int, str, int]], experiment_name: str) -> str:
    """
    Analyze the results and generate a report.
    Returns: Analysis text
    """
    lines = []
    lines.append("=" * 80)
    lines.append(f"ANALYSIS REPORT: {experiment_name}")
    lines.append("=" * 80)
    lines.append("")
    
    # Basic statistics
    lines.append(f"Total questions processed: {len(results)}")
    lines.append("")
    
    # Group by aggregator decision
    by_decision = defaultdict(list)
    for q_id, decision, score in results:
        by_decision[decision].append((q_id, score))
    
    # Overall score distribution
    lines.append("OVERALL SCORE DISTRIBUTION:")
    lines.append("-" * 40)
    score_counts = defaultdict(int)
    for _, _, score in results:
        if score >= 0:
            score_counts[score] += 1
    
    total_scored = sum(score_counts.values())
    for score in sorted(score_counts.keys()):
        count = score_counts[score]
        pct = (count / total_scored * 100) if total_scored > 0 else 0
        lines.append(f"  Score {score}: {count:4d} questions ({pct:5.2f}%)")
    
    if total_scored > 0:
        avg_score = sum(score * count for score, count in score_counts.items()) / total_scored
        lines.append(f"  Average score: {avg_score:.3f}")
    lines.append("")
    
    # Aggregator decision distribution
    lines.append("AGGREGATOR DECISION DISTRIBUTION:")
    lines.append("-" * 40)
    for decision in sorted(by_decision.keys()):
        count = len(by_decision[decision])
        pct = (count / len(results) * 100) if len(results) > 0 else 0
        lines.append(f"  {decision}: {count:4d} questions ({pct:5.2f}%)")
    lines.append("")
    
    # Score distribution by aggregator decision
    lines.append("SCORE DISTRIBUTION BY AGGREGATOR DECISION:")
    lines.append("=" * 80)
    
    for decision in sorted(by_decision.keys()):
        questions = by_decision[decision]
        lines.append("")
        lines.append(f"Decision: {decision.upper()}")
        lines.append(f"Total questions: {len(questions)}")
        lines.append("-" * 40)
        
        # Count scores
        decision_score_counts = defaultdict(int)
        for _, score in questions:
            if score >= 0:
                decision_score_counts[score] += 1
        
        total_decision_scored = sum(decision_score_counts.values())
        if total_decision_scored > 0:
            for score in sorted(decision_score_counts.keys()):
                count = decision_score_counts[score]
                pct = (count / total_decision_scored * 100)
                lines.append(f"  Score {score}: {count:4d} questions ({pct:5.2f}%)")
            
            avg = sum(score * count for score, count in decision_score_counts.items()) / total_decision_scored
            lines.append(f"  Average score: {avg:.3f}")
        else:
            lines.append("  No valid scores for this decision")
        
        # Show some example question IDs
        example_ids = [q[0] for q in questions[:10]]
        lines.append(f"  Example question IDs: {example_ids}")
    
    lines.append("")
    
    # Comparative analysis
    lines.append("COMPARATIVE ANALYSIS:")
    lines.append("-" * 40)
    
    # Average score per decision
    lines.append("\nAverage score by aggregator decision:")
    decision_avgs = []
    for decision in sorted(by_decision.keys()):
        questions = by_decision[decision]
        scores = [score for _, score in questions if score >= 0]
        if scores:
            avg = sum(scores) / len(scores)
            decision_avgs.append((decision, avg, len(scores)))
            lines.append(f"  {decision}: avg score = {avg:.3f} (n={len(scores)})")
    
    # Identify best performing decision
    lines.append("\nBest performing aggregator decision:")
    if decision_avgs:
        best_decision, best_avg, best_n = max(decision_avgs, key=lambda x: x[1])
        lines.append(f"  {best_decision} with average score {best_avg:.3f} (n={best_n})")
    
    # Score 2 (correct) analysis
    lines.append("\nPerfect scores (score=2) by decision:")
    for decision in sorted(by_decision.keys()):
        questions = by_decision[decision]
        perfect = sum(1 for _, score in questions if score == 2)
        total = len(questions)
        pct = (perfect / total * 100) if total > 0 else 0
        lines.append(f"  {decision}: {perfect}/{total} ({pct:.2f}%)")
    
    # Score 0 (incorrect) analysis
    lines.append("\nIncorrect answers (score=0) by decision:")
    for decision in sorted(by_decision.keys()):
        questions = by_decision[decision]
        incorrect = sum(1 for _, score in questions if score == 0)
        total = len(questions)
        pct = (incorrect / total * 100) if total > 0 else 0
        lines.append(f"  {decision}: {incorrect}/{total} ({pct:.2f}%)")
    
    # Recommendation
    lines.append("\nRECOMMENDATION:")
    if decision_avgs:
        sorted_decisions = sorted(decision_avgs, key=lambda x: x[1], reverse=True)
        lines.append(f"  Based on average scores, the ranking is:")
        for i, (dec, avg, n) in enumerate(sorted_decisions, 1):
            lines.append(f"    {i}. {dec} (avg={avg:.3f}, n={n})")
        
        # Analysis insights
        if len(sorted_decisions) >= 2:
            best = sorted_decisions[0]
            second = sorted_decisions[1]
            diff = best[1] - second[1]
            if diff > 0.1:
                lines.append(f"\n  {best[0]} significantly outperforms other decisions (+{diff:.3f} vs {second[0]})")
            elif diff > 0.05:
                lines.append(f"\n  {best[0]} moderately outperforms other decisions (+{diff:.3f} vs {second[0]})")
            else:
                lines.append(f"\n  Performance is similar across decisions (max diff: {diff:.3f})")
    
    lines.append("")
    lines.append("=" * 80)
    
    return "\n".join(lines)


def main():
    print("Starting Aggregator Decision analysis...")
    print(f"CSV file: {CSV_FILE}")
    print(f"Layer 1 folder: {LAYER1_FOLDER}")
    print(f"Output directory: {OUTPUT_DIR}")
    print()
    
    # Create output directory
    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
    
    # Load scores from CSV
    print("Loading scores from CSV...")
    all_scores = load_scores_from_csv(CSV_FILE)
    print(f"Loaded scores for {len(all_scores)} experiments")
    print()
    
    # Process each experiment folder
    for folder_name, column_name in FOLDER_TO_COLUMN.items():
        print(f"Processing experiment: {folder_name}")
        print(f"  CSV column: {column_name}")
        
        folder_path = LAYER1_FOLDER / folder_name
        if not folder_path.exists():
            print(f"  ERROR: Folder not found: {folder_path}")
            print()
            continue
        
        # Get scores for this experiment
        scores = all_scores[column_name]
        
        # Process log files
        results = process_experiment_folder(folder_path, column_name, scores)
        print(f"  Processed {len(results)} questions successfully")
        
        # Save results to CSV
        results_file = OUTPUT_DIR / f"{folder_name}_results.csv"
        with open(results_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['question_id', 'aggregator_decision', 'llm_judge_score'])
            for q_id, decision, score in sorted(results):
                writer.writerow([q_id, decision, score])
        print(f"  Saved results to: {results_file}")
        
        # Generate and save analysis
        analysis = analyze_results(results, folder_name)
        analysis_file = OUTPUT_DIR / f"{folder_name}_analysis.txt"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write(analysis)
        print(f"  Saved analysis to: {analysis_file}")
        
        # Also print analysis to console
        print()
        print(analysis)
        print()
    
    print("Aggregator Decision analysis complete!")


if __name__ == "__main__":
    main()