#!/usr/bin/env python3
"""
parse_logs_to_jsonl.py

Parses log files generated by lexidkg_graphrag_agentic.py and creates a JSONL file
with question, ground_truth (if available), and generated_answer fields.
"""

import json
import re
from pathlib import Path
from typing import Optional, Dict, Any

# ===========================
# HARDCODED CONFIGURATION
# ===========================
LOG_DIR = Path("question_terminal_logs/lexidkg_4_iq_fix")  # Directory containing log files (.txt)
OUTPUT_FILE = Path("./output.jsonl")  # Output JSONL file path
GROUND_TRUTH_FILE = "../../../dataset/4_experiment/4a_qa_generation/4a_iii_qa_pairs_with_id/qa_pairs.jsonl"  # Optional: Path("./ground_truth.jsonl") or None

# ===========================


def extract_original_query(log_content: str) -> Optional[str]:
    """Extract the original query from the log file."""
    # Look for "Original Query: ..." line
    match = re.search(r'Original Query:\s*(.+?)(?:\n|$)', log_content)
    if match:
        return match.group(1).strip()
    return None


def extract_final_answer(log_content: str) -> Optional[str]:
    """
    Extract the final answer ONLY from the "=== Final Answer ===" section.
    No fallbacks to other sections.
    
    Returns:
        The final answer text, or None if not found.
    """
    # Look for the section after "=== Final Answer ===" or "=== Final Answer (from last IQ) ==="
    # Pattern captures text between the header and the next log marker or "Logs saved to:"
    patterns = [
        r'=== Final Answer \(from last IQ\) ===\s*\n\[.*?\]\s*\[INFO\].*?\]\s*(.+?)(?=\n\[.*?\]\s*\[INFO\].*?Logs saved to:)',
        r'=== Final Answer ===\s*\n\[.*?\]\s*\[INFO\].*?\]\s*(.+?)(?=\n\[.*?\]\s*\[INFO\].*?Logs saved to:)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, log_content, re.DOTALL)
        if match:
            answer = match.group(1).strip()
            # Remove any log line artifacts that might have leaked through
            answer = re.sub(r'\[.*?\]\s*\[(?:INFO|WARN|DEBUG|ERROR)\].*?\].*', '', answer).strip()
            if answer:  # Only return if we have actual content
                return answer
    
    return None


def parse_log_file(log_path: Path) -> Optional[Dict[str, Any]]:
    """
    Parse a single log file and extract question and generated answer.
    
    Returns:
        Dict with 'question' and 'generated_answer' keys, or None if parsing fails.
    """
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        question = extract_original_query(content)
        answer = extract_final_answer(content)
        
        if not question:
            print(f"ERROR: Could not extract question from {log_path.name}")
            return None
        
        if not answer:
            print(f"ERROR: Could not extract final answer from {log_path.name}")
            return None
        
        return {
            'question': question,
            'generated_answer': answer
        }
    
    except Exception as e:
        print(f"ERROR parsing {log_path.name}: {e}")
        return None


def main():
    if not LOG_DIR.exists() or not LOG_DIR.is_dir():
        print(f"ERROR: {LOG_DIR} is not a valid directory")
        return
    
    # Load ground truth if provided
    ground_truth_map = {}
    if GROUND_TRUTH_FILE:
        gt_path = Path(GROUND_TRUTH_FILE)
        if gt_path.exists():
            print(f"Loading ground truth from {gt_path}")
            with open(gt_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        gt_item = json.loads(line)
                        # Map by question text (normalized)
                        question_key = gt_item['question'].strip().lower()
                        ground_truth_map[question_key] = gt_item.get('ground_truth', '')
            print(f"Loaded {len(ground_truth_map)} ground truth entries")
        else:
            print(f"WARNING: Ground truth file {gt_path} not found")
    
    # Find all .txt log files
    log_files = sorted(LOG_DIR.glob('*.txt'))
    print(f"Found {len(log_files)} log files in {LOG_DIR}")
    
    results = []
    success_count = 0
    error_count = 0
    
    for idx, log_file in enumerate(log_files):
        parsed = parse_log_file(log_file)
        
        if parsed:
            # Create record with id
            record = {
                'id': idx,
                'question': parsed['question'],
            }
            
            # Add ground truth if available
            question_key = parsed['question'].strip().lower()
            if question_key in ground_truth_map:
                record['ground_truth'] = ground_truth_map[question_key]
            else:
                # You can set a default or leave it out
                record['ground_truth'] = ''  # or you can omit this line to exclude the field
            
            record['generated_answer'] = parsed['generated_answer']
            
            results.append(record)
            success_count += 1
        else:
            error_count += 1
    
    # Write to JSONL
    print(f"\n{'='*60}")
    print(f"Summary:")
    print(f"  Successfully parsed: {success_count}/{len(log_files)} log files")
    print(f"  Errors: {error_count}/{len(log_files)} log files")
    print(f"{'='*60}")
    
    if results:
        print(f"\nWriting to {OUTPUT_FILE}")
        OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for record in results:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        print(f"Done! Wrote {len(results)} records to {OUTPUT_FILE}")
    else:
        print("\nNo records to write. Please check the errors above.")


if __name__ == '__main__':
    main()